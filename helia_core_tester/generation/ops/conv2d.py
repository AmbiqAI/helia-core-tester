"""
Conv2D operation implementation.
"""

from typing import Dict
from pathlib import Path
import numpy as np
import tensorflow as tf
from helia_core_tester.generation.ops.base import OperationBase


class OpConv2D(OperationBase):
    """
    Conv2D operation.
    """
    
    def build_keras_model(self) -> tf.keras.Model:
        input_shape = self.desc['input_shape']
        filter_shape = self.desc['filter_shape']
        
        tf.keras.utils.set_random_seed(17)
        
        padding = self.desc.get('padding', 'valid')
        if padding is not None:
            padding = str(padding).lower()
        else:
            padding = 'valid'
        
        activation = self.desc.get('activation', 'NONE')
        act = None if activation in (None, 'NONE', 'none') else activation.lower()
        
        dilation = self.desc.get('dilation', [1, 1])
        if isinstance(dilation, (int, float)):
            dilation = [int(dilation), int(dilation)]
        elif isinstance(dilation, (list, tuple)):
            if len(dilation) != 2:
                raise ValueError(f"Invalid dilation: {dilation}. Must be 2 integers or a single integer")
            dilation = [int(dilation[0]), int(dilation[1])]
        
        if any(d <= 0 for d in dilation):
            raise ValueError(f"Invalid dilation values: {dilation}. Must be positive integers")
        
        x = tf.keras.Input(
            shape=input_shape[1:],
            batch_size=input_shape[0] if len(input_shape) > 0 else None,
            dtype=tf.float32,
            name='input'
        )
        
        conv = tf.keras.layers.Conv2D(
            filters=filter_shape[3],
            kernel_size=tuple(filter_shape[0:2]),
            strides=tuple(self.desc.get('strides', [1, 1])),
            dilation_rate=tuple(dilation),
            padding=padding,
            use_bias=self.desc.get('use_bias', True),
            activation=act,
            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1234),
            bias_initializer='zeros',
            name='conv_2d'
        )(x)
        
        model = tf.keras.Model(inputs=[x], outputs=conv, name='conv_2d')
        return model

    def convert_to_tflite(self, model, out_path: str, rep_seed: int) -> None:
        """Convert Keras model to TFLite with quantization."""
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        
        activation_dtype = self.desc.get('activation_dtype', 'S8')
        
        if activation_dtype == 'S8':
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.int8]
            converter.inference_input_type = tf.int8
            converter.inference_output_type = tf.int8
        elif activation_dtype == 'S16':
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
            ]
            converter.inference_input_type = tf.int16
            converter.inference_output_type = tf.int16
        
        def representative_data_gen():
            rep_rng = np.random.default_rng(42)
            for _ in range(100):
                if 'input_shape' in self.desc:
                    inputs = rep_rng.integers(-32, 32, size=self.desc['input_shape']).astype(np.float32)
                    yield [inputs]
                elif 'input_1_shape' in self.desc and 'input_2_shape' in self.desc:
                    inputs1 = rep_rng.integers(-32, 32, size=self.desc['input_1_shape']).astype(np.float32)
                    inputs2 = rep_rng.integers(-32, 32, size=self.desc['input_2_shape']).astype(np.float32)
                    yield [inputs1, inputs2]
        
        converter.representative_dataset = representative_data_gen
        
        tflite_model = converter.convert()
        with open(out_path, 'wb') as f:
            f.write(tflite_model)
            
    def _select_cmsis_conv2d_kernel(self) -> Dict[str, str]:
        """
        Pick CMSIS-NN wrapper + C types from descriptor dtypes.

        Conv2D support here is limited to:
        - S8 activations, S8 weights  -> arm_convolve_wrapper_s8, int32 bias
        - S16 activations, S8 weights -> arm_convolve_wrapper_s16, int64 bias

        Weight dtype S4 is not generated by the current TF->TFLite path, so we block it early.
        """
        act = str(self.desc.get("activation_dtype", "S8")).upper()
        w = str(self.desc.get("weight_dtype", "S8")).upper()

        if w == "S4":
            raise NotImplementedError("Conv2D with S4 weights is not supported by the current generator (no int4 weight packing).")

        if act == "S8" and w == "S8":
            return {
                "kernel_fn": "arm_convolve_wrapper_s8",
                "kernel_get_buffer_size_fn": "arm_convolve_wrapper_s8_get_buffer_size",
                "input_c_type": "int8_t",
                "output_c_type": "int8_t",
                "bias_c_type": "int32_t",
            }

        if act == "S16" and w == "S8":
            return {
                "kernel_fn": "arm_convolve_wrapper_s16",
                "kernel_get_buffer_size_fn": "arm_convolve_wrapper_s16_get_buffer_size",
                "input_c_type": "int16_t",
                "output_c_type": "int16_t",
                "bias_c_type": "int64_t",
            }

        raise NotImplementedError(f"Unsupported Conv2D dtype combo: {act} x {w}")

    def generate_c_files(self, output_dir: Path) -> None:
        """
        Generate C and H files from templates for Conv2D operation.
        """
        from helia_core_tester.generation.utils.template_context import TemplateContextBuilder

        name = self.desc['name']
        tflite_path = output_dir / f"{name}.tflite"
        if not tflite_path.exists():
            raise FileNotFoundError(f"TFLite file not found: {tflite_path}")

        # Select CMSIS kernel + types
        kernel_info = self._select_cmsis_conv2d_kernel()

        # Load LiteRT model for tensor extraction
        from helia_core_tester.generation.utils.litert_utils import (
            load_litert_model,
            get_operator_tensors_from_litert,
            get_tensor_shape_from_litert,
            get_tensor_quantization_from_litert,
        )
        
        model, subgraph = load_litert_model(str(tflite_path))
        
        # Pick the Conv2D operator index.
        # Dilation is commonly lowered to SpaceToBatchND -> Conv2D -> BatchToSpaceND,
        # so the first operator is not necessarily Conv2D.
        if len(subgraph.operators) == 0:
            raise ValueError("No operators found in model")

        conv_op_index = 0
        try:
            from ai_edge_litert import schema_py_generated as litert
            for i, op in enumerate(subgraph.operators):
                opcode = model.operatorCodes[op.opcodeIndex]
                if opcode.builtinCode == litert.BuiltinOperator.CONV_2D:
                    conv_op_index = i
                    break
        except Exception:
            # Fallback to first op if we cannot inspect opcodes.
            conv_op_index = 0

        op_tensors = get_operator_tensors_from_litert(model, subgraph, conv_op_index)
        
        # Extract shapes from LiteRT.
        # Prefer subgraph I/O shapes (model input/output), not Conv2D op I/O,
        # because dilation lowering changes intermediate shapes.
        input_tensor = None
        output_tensor = None
        if subgraph.inputs and subgraph.outputs:
            input_tensor = subgraph.tensors[subgraph.inputs[0]]
            output_tensor = subgraph.tensors[subgraph.outputs[0]]
        else:
            # Fallback: try to map interpreter I/O tensor indices to LiteRT tensors.
            try:
                interpreter = self.load_litert_interpreter(str(tflite_path))
                in_details = interpreter.get_input_details()
                out_details = interpreter.get_output_details()
                if in_details:
                    in_idx = int(in_details[0]["index"])
                    if 0 <= in_idx < len(subgraph.tensors):
                        input_tensor = subgraph.tensors[in_idx]
                if out_details:
                    out_idx = int(out_details[0]["index"])
                    if 0 <= out_idx < len(subgraph.tensors):
                        output_tensor = subgraph.tensors[out_idx]
            except Exception:
                # Will fall back to op tensors below.
                pass

        input_shape = get_tensor_shape_from_litert(input_tensor) if input_tensor is not None else None
        output_shape = get_tensor_shape_from_litert(output_tensor) if output_tensor is not None else None
        if input_shape is None and op_tensors['inputs']:
            input_shape = op_tensors['inputs'][0]['shape']
        if output_shape is None and op_tensors['outputs']:
            output_shape = op_tensors['outputs'][0]['shape']
        if input_shape is None or output_shape is None:
            raise ValueError("Missing input/output shapes from LiteRT (subgraph + interpreter + op tensors)")
        
        # Extract quantization parameters.
        # Use subgraph I/O quantization for input/output to match model I/O.
        if input_tensor is not None:
            input_quant = get_tensor_quantization_from_litert(input_tensor)
        elif op_tensors['inputs']:
            input_quant = op_tensors['inputs'][0]['quantization']
        else:
            input_quant = None

        if output_tensor is not None:
            output_quant = get_tensor_quantization_from_litert(output_tensor)
        elif op_tensors['outputs']:
            output_quant = op_tensors['outputs'][0]['quantization']
        else:
            output_quant = None
        
        # Find weight quantization (from weight tensor in inputs)
        weight_quant = None
        for input_tensor_info in op_tensors['inputs']:
            if input_tensor_info['data'] is not None and len(input_tensor_info['shape']) > 1:
                weight_quant = input_tensor_info['quantization']
                break
        
        quant_params = {
            'input': input_quant or {'scale': 1.0, 'zero_point': 0, 'per_channel': False},
            'output': output_quant or {'scale': 1.0, 'zero_point': 0, 'per_channel': False},
            'weight': weight_quant or input_quant or {'scale': 1.0, 'zero_point': 0, 'per_channel': False}
        }
        
        # Extract weights and biases from LiteRT
        weights = op_tensors['weights']
        biases = op_tensors['biases']

        # Weight tensor for TFLite Conv2D is OHWI in practice; shape will be (O, H, W, I)
        if weights is not None:
            filter_shape = tuple(weights.shape)
            # Ensure weights are int8 in generated C
            if weights.dtype != np.int8:
                weights = weights.astype(np.int8)
        else:
            # Fallback: descriptor is HWIO (kh, kw, in, out)
            fs = tuple(self.desc['filter_shape'])
            filter_shape = (fs[3], fs[0], fs[1], fs[2])  # OHWI

        builder = TemplateContextBuilder()
        input_dims = builder.nhwc_to_cmsis_dims(input_shape)
        output_dims = builder.nhwc_to_cmsis_dims(output_shape)

        # CMSIS expects OHWI dims
        # For grouped convolutions, CMSIS calculates groups = input_ch / filter_ch
        # TFLite stores filters with input_ch channels (all groups), but CMSIS expects input_ch/groups
        # So we need to adjust filter_dims.c for grouped convolutions
        groups = self.desc.get('groups', 1)
        filter_dims = {
            'n': int(filter_shape[0]),
            'h': int(filter_shape[1]),
            'w': int(filter_shape[2]),
            'c': int(filter_shape[3]),  # Use full input channels, NOT divided by groups
        }

        # Correct kernel size for padding math
        kernel_hw = (filter_dims['h'], filter_dims['w'])

        # Build convolution parameters (fix SAME padding + offsets)
        conv_params = builder.build_conv_params(
            self.desc,
            input_shape,
            kernel_hw,
            output_shape,
            quant_params['input'],
            quant_params['output']
        )

        # Build quantization parameters
        # CRITICAL: The effective scale for multiplier/shift is NOT output_scale directly!
        # It should be: effective_scale = (input_scale * weight_scale) / output_scale
        # This matches CMSIS-NN test code in op_utils.py line 194
        input_quant = quant_params['input']
        output_quant = quant_params['output']
        weight_quant = quant_params.get('weight', output_quant)
        
        input_scale = input_quant.get('scale', 1.0)
        if isinstance(input_scale, (list, np.ndarray)):
            input_scale = float(input_scale[0])
        else:
            input_scale = float(input_scale)
        
        output_scale = output_quant.get('scale', 1.0)
        if isinstance(output_scale, (list, np.ndarray)):
            output_scale = float(output_scale[0])
        else:
            output_scale = float(output_scale)
        
        weight_scale = weight_quant.get('scale', 1.0)
        per_channel = bool(weight_quant.get('per_channel', False))
        
        # Calculate effective scales: (input_scale * weight_scale) / output_scale
        if per_channel and isinstance(weight_scale, np.ndarray):
            # Per-channel: effective_scale[i] = (input_scale * weight_scale[i]) / output_scale
            effective_scales = (input_scale * weight_scale) / output_scale
            # Create a temporary quant_params dict with effective scales
            effective_quant = {
                'scale': effective_scales,
                'zero_point': output_quant.get('zero_point', 0),
                'per_channel': True
            }
            quant_params_dict = builder.build_quant_params(effective_quant, per_channel=True)
            # Store effective scales for logging
            quant_params_dict['effective_scales'] = effective_scales
            # Store raw multiplier and shift arrays for logging (before formatting)
            from helia_core_tester.generation.utils.tflite_utils import calculate_per_channel_multiplier_shift
            multipliers_raw, shifts_raw = calculate_per_channel_multiplier_shift(effective_scales)
            quant_params_dict['multiplier_array_raw'] = multipliers_raw
            quant_params_dict['shift_array_raw'] = shifts_raw
        else:
            # Per-tensor: effective_scale = (input_scale * weight_scale) / output_scale
            if isinstance(weight_scale, (list, np.ndarray)):
                weight_scale = float(weight_scale[0])
            else:
                weight_scale = float(weight_scale)
            effective_scale = (input_scale * weight_scale) / output_scale
            # Ensure effective_scale is a Python float, not numpy scalar
            effective_scale = float(effective_scale)
            effective_quant = {
                'scale': effective_scale,
                'zero_point': output_quant.get('zero_point', 0),
                'per_channel': False
            }
            quant_params_dict = builder.build_quant_params(effective_quant, per_channel=False)
            # Store effective scale for logging
            quant_params_dict['effective_scale'] = effective_scale
            # Store raw multiplier and shift for logging (before formatting)
            from helia_core_tester.generation.utils.tflite_utils import calculate_multiplier_shift
            multiplier_raw, shift_raw = calculate_multiplier_shift(effective_scale)
            quant_params_dict['multiplier_raw'] = multiplier_raw
            quant_params_dict['shift_raw'] = shift_raw
        
        quant_params_dict['per_channel'] = per_channel

        # Generate input data and quantize to the interpreter's real input dtype
        # IMPORTANT: Reset RNG to seed to ensure input data matches what was used
        # during TFLite conversion (representative dataset generation may have advanced RNG)
        rng_state = self.rng.__getstate__()
        self.rng = np.random.default_rng(self.seed)
        input_data = self.generate_input_data()
        self.rng.__setstate__(rng_state)
        
        input_scale = quant_params['input'].get('scale', 1.0)
        input_zp = quant_params['input'].get('zero_point', 0)
        if isinstance(input_scale, (list, np.ndarray)):
            input_scale = input_scale[0]

        if kernel_info["input_c_type"] == "int8_t":
            qmin, qmax = -128, 127
            np_in_dtype = np.int8
        elif kernel_info["input_c_type"] == "int16_t":
            qmin, qmax = -32768, 32767
            np_in_dtype = np.int16
        else:
            raise ValueError(f"Unsupported input_c_type: {kernel_info['input_c_type']}")

        input_q = np.round(input_data / float(input_scale) + float(input_zp)).astype(np.int32)
        input_q = np.clip(input_q, qmin, qmax).astype(np_in_dtype)

        # Run inference (dtype must match interpreter input)
        output_data = self.run_inference(str(tflite_path), input_q)

        # Bias handling
        # Detect bias dtype from the LiteRT tensor and use it for CMSIS-NN.
        # This is critical for S16 convs where int32 vs int64 bias selects
        # different requantization paths.
        has_biases = biases is not None and getattr(biases, "size", 0) > 0
        bias_dtype = kernel_info["bias_c_type"]
        if has_biases:
            if biases.dtype == np.int32:
                bias_dtype = "int32_t"
            elif biases.dtype == np.int64:
                bias_dtype = "int64_t"
            else:
                # Fallback: cast to the kernel's expected bias type
                if bias_dtype == "int64_t":
                    biases = biases.astype(np.int64)
                else:
                    biases = biases.astype(np.int32)

            # Ensure actual array dtype matches the chosen bias_dtype
            if bias_dtype == "int64_t" and biases.dtype != np.int64:
                biases = biases.astype(np.int64)
            elif bias_dtype == "int32_t" and biases.dtype != np.int32:
                biases = biases.astype(np.int32)

        # Format arrays
        weights_array_str = builder.format_array_as_c_literal(weights) if weights is not None else ""
        biases_array_str = builder.format_array_as_c_literal(biases) if has_biases else ""
        input_data_array_str = builder.format_array_as_c_literal(input_q)
        expected_output_array_str = builder.format_array_as_c_literal(output_data)

        # Calculate buffer size max (conservative estimate)
        # Use activation_dtype to determine if this is S8 or S16 convolution
        activation_dtype = self.desc.get('activation_dtype', 'S8')
        buffer_size_max = builder.calculate_buffer_size_max(
            input_dims, filter_dims, output_dims, 
            output_dtype=activation_dtype
        )

        # Build template context
        context = {
            'name': name,
            'prefix': name,
            'input_dims': input_dims,
            'filter_dims': filter_dims,
            'output_dims': output_dims,
            'conv_params': conv_params,
            'quant_params': quant_params_dict,
            'weights_array': weights_array_str,
            'biases_array': biases_array_str,
            'has_biases': has_biases,
            'input_data_array': input_data_array_str,
            'expected_output_array': expected_output_array_str,
            'input_dtype': kernel_info["input_c_type"],
            'output_dtype': kernel_info["output_c_type"],
            'bias_dtype': bias_dtype,
            'kernel_fn': kernel_info["kernel_fn"],
            'kernel_get_buffer_size_fn': kernel_info["kernel_get_buffer_size_fn"],
            'buffer_size_max': buffer_size_max,
        }

        # Render templates
        includes_api_dir = output_dir / "includes"
        includes_api_dir.mkdir(parents=True, exist_ok=True)

        h_content = self.render_template("conv2d/conv2d.h.j2", context)
        h_path = includes_api_dir / f"{name}_conv2d.h"
        with open(h_path, 'w') as f:
            f.write(h_content)

        c_content = self.render_template("conv2d/conv2d.c.j2", context)
        c_path = output_dir / f"{name}_conv2d.c"
        with open(c_path, 'w') as f:
            f.write(c_content)

        cmake_context = {
            'name': name,
            'operator': self.desc.get('operator', 'Conv2D'),
            'operator_name': 'conv2d'
        }
        cmake_content = self.render_template("common/CMakeLists.txt.j2", cmake_context)
        cmake_path = output_dir / "CMakeLists.txt"
        with open(cmake_path, 'w') as f:
            f.write(cmake_content)

        print(f"Generated C/H files and CMakeLists.txt for {name}")
